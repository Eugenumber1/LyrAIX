{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8fd4090-c6f1-4ae4-8511-b577a43bbbdd",
   "metadata": {},
   "source": [
    "## Topic Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a0affc0-6999-4e2d-a93b-4ef26f83876b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "lyrics = pd.read_csv('song_parts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f982d53-c088-48a2-82ac-8675d179a8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "lyric_corpus = lyrics['lyrics']\n",
    "lyric_corpus_tokenized = []\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "for lyric in lyric_corpus:\n",
    "    tokenized_lyric = tokenizer.tokenize(lyric.lower())\n",
    "    lyric_corpus_tokenized.append(tokenized_lyric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e4dc5fa-1d18-485e-ab9a-a704a2a1b0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for s,song in enumerate(lyric_corpus_tokenized):\n",
    "    filtered_song = []\n",
    "    for token in song:\n",
    "        if len(token) > 2 and not token.isnumeric():\n",
    "            filtered_song.append(token)\n",
    "    lyric_corpus_tokenized[s] = filtered_song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3522e07c-0fd0-4f2a-b9e6-e9f6ba077b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lemmatizing lyrics: 100%|██████████| 5334078/5334078 [15:42<00:00, 5659.74it/s] \n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from tqdm import tqdm\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for s, song in enumerate(tqdm(lyric_corpus_tokenized, desc=\"Lemmatizing lyrics\")):\n",
    "    lemmatized_tokens = []\n",
    "    for token in song:\n",
    "        lemmatized_tokens.append(lemmatizer.lemmatize(token))\n",
    "    lyric_corpus_tokenized[s] = lemmatized_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec237dbd-b044-4386-b0af-1cf4dc9b2ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['while', 'watchin', 'every', 'nigga', 'watchin', 'closely', 'shit', 'butter', 'for', 'the', 'bread', 'they', 'wanna', 'toast', 'keep', 'head', 'both', 'them', 'where', 'they', 'supposed', 'hoe', 'get', 'you', 'sidetracked', 'then', 'clapped', 'from', 'close', 'foot', 'don', 'sleep', 'tired', 'feel', 'wired', 'like', 'codeine', 'these', 'day', 'brother', 'gotta', 'admire', 'from', 'four', 'fiend', 'away', 'pain', 'wish', 'wa', 'quick', 'see', 'from', 'sellin', 'caine', 'til', 'brain', 'wa', 'fried', 'fricassee', 'can', 'lie', 'the', 'time', 'never', 'bothered', 'the', 'bar', 'gettin', 'thug', 'properly', 'squad', 'and', 'lack', 'respect', 'for', 'authority', 'laughin', 'hard', 'happy', 'escapin', 'poverty', 'however', 'brief', 'know', 'this', 'game', 'got', 'valley', 'and', 'peak', 'expectation', 'for', 'dip', 'for', 'precipitation', 'stack', 'chip', 'hardly', 'the', 'youth', 'used', 'soon', 'see', 'mill', 'more', 'big', 'willie', 'game', 'ha', 'grown', 'prefer', 'you', 'call', 'william', 'illin', 'for', 'revenue', 'rayful', 'edmond', 'like', 'channel', 'news', 'round', 'seven', 'jewel', 'head', 'dead', 'the', 'mic', 'forgettin', 'all', 'ever', 'knew', 'convenient', 'amnesia', 'suggest', 'you', 'call', 'lawyer', 'know', 'the', 'procedure', 'lock', 'body', 'can', 'trap', 'mind', 'easily', 'explain', 'why', 'adapt', 'crime', 'rather', 'die', 'enormous', 'than', 'live', 'dormant', 'that', 'how', 'live', 'the', 'main', 'event', 'bet', 'trip', 'maui', 'presidential', 'suite', 'residential', 'for', 'the', 'weekend', 'confidentially', 'speakin', 'code', 'since', 'sense', 'you', 'peekin', 'the', 'nsx', 'rental', 'don', 'fooled', 'game', 'mental', 'both', 'out', 'town', 'dog', 'what', 'you', 'tryin', 'get', 'into', 'viva', 'la', 'vega', 'see', 'later', 'the', 'crap', 'table', 'meet', 'the', 'one', 'that', 'start', 'this', 'way', 'fraud', 'willies', 'present', 'gamblin', 'they', 'and', 'can', 'have', 'pleasant', 'time', 'sippin', 'margarita']\n"
     ]
    }
   ],
   "source": [
    "print(lyric_corpus_tokenized[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f2c7d4c-aee4-4700-aaec-4ef597256f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics['lemmatized'] = lyric_corpus_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1e780d09-c77d-448f-89bb-403eaae5904f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>title</th>\n",
       "      <th>tag</th>\n",
       "      <th>year</th>\n",
       "      <th>views</th>\n",
       "      <th>part</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>explicitness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>JAY-Z</td>\n",
       "      <td>Can I Live</td>\n",
       "      <td>rap</td>\n",
       "      <td>1996</td>\n",
       "      <td>468624</td>\n",
       "      <td>[Verse 1]</td>\n",
       "      <td>While I'm watchin' every nigga watchin' me clo...</td>\n",
       "      <td>Explicit content</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>JAY-Z</td>\n",
       "      <td>Can I Live</td>\n",
       "      <td>rap</td>\n",
       "      <td>1996</td>\n",
       "      <td>468624</td>\n",
       "      <td>[Chorus]</td>\n",
       "      <td>Ge-ge-geyeahhh\\nCan I live?\\nCan I live?</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>JAY-Z</td>\n",
       "      <td>Can I Live</td>\n",
       "      <td>rap</td>\n",
       "      <td>1996</td>\n",
       "      <td>468624</td>\n",
       "      <td>[Verse 2]</td>\n",
       "      <td>My mind is infested with sick thoughts that ci...</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>JAY-Z</td>\n",
       "      <td>Can I Live</td>\n",
       "      <td>rap</td>\n",
       "      <td>1996</td>\n",
       "      <td>468624</td>\n",
       "      <td>[Chorus]</td>\n",
       "      <td>Can I live?\\nCan I live?\\nCan I live?\\nCan I l...</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fabolous</td>\n",
       "      <td>Forgive Me Father</td>\n",
       "      <td>rap</td>\n",
       "      <td>2003</td>\n",
       "      <td>4743</td>\n",
       "      <td>[Hook]</td>\n",
       "      <td>Forgive me father for I have sinned\\nBut look ...</td>\n",
       "      <td>Explicit content</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5334073</th>\n",
       "      <td>Alana Springsteen</td>\n",
       "      <td>New Number</td>\n",
       "      <td>country</td>\n",
       "      <td>2022</td>\n",
       "      <td>1</td>\n",
       "      <td>[Chorus]</td>\n",
       "      <td>One that I ain't dial at least a couple thousa...</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5334074</th>\n",
       "      <td>Alana Springsteen</td>\n",
       "      <td>New Number</td>\n",
       "      <td>country</td>\n",
       "      <td>2022</td>\n",
       "      <td>1</td>\n",
       "      <td>[Verse 2]</td>\n",
       "      <td>You need a new number and you can't get it fas...</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5334075</th>\n",
       "      <td>Alana Springsteen</td>\n",
       "      <td>New Number</td>\n",
       "      <td>country</td>\n",
       "      <td>2022</td>\n",
       "      <td>1</td>\n",
       "      <td>[Chorus]</td>\n",
       "      <td>One that I ain't dial at least a couple thousa...</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5334076</th>\n",
       "      <td>Alana Springsteen</td>\n",
       "      <td>New Number</td>\n",
       "      <td>country</td>\n",
       "      <td>2022</td>\n",
       "      <td>1</td>\n",
       "      <td>[Bridge]</td>\n",
       "      <td>Oh, if you wanna help me out\\nIf you wanna let...</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5334077</th>\n",
       "      <td>Alana Springsteen</td>\n",
       "      <td>New Number</td>\n",
       "      <td>country</td>\n",
       "      <td>2022</td>\n",
       "      <td>1</td>\n",
       "      <td>[Verse 3]</td>\n",
       "      <td>You need a new number, one that I don't know\\n...</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5334078 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    artist              title      tag  year   views  \\\n",
       "0                    JAY-Z         Can I Live      rap  1996  468624   \n",
       "1                    JAY-Z         Can I Live      rap  1996  468624   \n",
       "2                    JAY-Z         Can I Live      rap  1996  468624   \n",
       "3                    JAY-Z         Can I Live      rap  1996  468624   \n",
       "4                 Fabolous  Forgive Me Father      rap  2003    4743   \n",
       "...                    ...                ...      ...   ...     ...   \n",
       "5334073  Alana Springsteen         New Number  country  2022       1   \n",
       "5334074  Alana Springsteen         New Number  country  2022       1   \n",
       "5334075  Alana Springsteen         New Number  country  2022       1   \n",
       "5334076  Alana Springsteen         New Number  country  2022       1   \n",
       "5334077  Alana Springsteen         New Number  country  2022       1   \n",
       "\n",
       "              part                                             lyrics  \\\n",
       "0        [Verse 1]  While I'm watchin' every nigga watchin' me clo...   \n",
       "1         [Chorus]           Ge-ge-geyeahhh\\nCan I live?\\nCan I live?   \n",
       "2        [Verse 2]  My mind is infested with sick thoughts that ci...   \n",
       "3         [Chorus]  Can I live?\\nCan I live?\\nCan I live?\\nCan I l...   \n",
       "4           [Hook]  Forgive me father for I have sinned\\nBut look ...   \n",
       "...            ...                                                ...   \n",
       "5334073   [Chorus]  One that I ain't dial at least a couple thousa...   \n",
       "5334074  [Verse 2]  You need a new number and you can't get it fas...   \n",
       "5334075   [Chorus]  One that I ain't dial at least a couple thousa...   \n",
       "5334076   [Bridge]  Oh, if you wanna help me out\\nIf you wanna let...   \n",
       "5334077  [Verse 3]  You need a new number, one that I don't know\\n...   \n",
       "\n",
       "             explicitness  \n",
       "0        Explicit content  \n",
       "1                  Normal  \n",
       "2                  Normal  \n",
       "3                  Normal  \n",
       "4        Explicit content  \n",
       "...                   ...  \n",
       "5334073            Normal  \n",
       "5334074            Normal  \n",
       "5334075            Normal  \n",
       "5334076            Normal  \n",
       "5334077            Normal  \n",
       "\n",
       "[5334078 rows x 8 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4fbf329c-f4cc-4d2c-a7ba-ec9578fa6455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lyrics['tokenized, lemmatized, no bad words'] = lyric_corpus_tokenized\n",
    "lyrics.to_csv('song_lyrics_lemmatized.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "53b3dfd4-d502-43d6-a7ae-ad4111b530c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>title</th>\n",
       "      <th>tag</th>\n",
       "      <th>year</th>\n",
       "      <th>views</th>\n",
       "      <th>part</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>explicitness</th>\n",
       "      <th>tokenized, lemmatized, no bad words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>JAY-Z</td>\n",
       "      <td>Can I Live</td>\n",
       "      <td>rap</td>\n",
       "      <td>1996</td>\n",
       "      <td>468624</td>\n",
       "      <td>[Verse 1]</td>\n",
       "      <td>While I'm watchin' every nigga watchin' me clo...</td>\n",
       "      <td>Explicit content</td>\n",
       "      <td>[watchin, every, watchin, closely, butter, bre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>JAY-Z</td>\n",
       "      <td>Can I Live</td>\n",
       "      <td>rap</td>\n",
       "      <td>1996</td>\n",
       "      <td>468624</td>\n",
       "      <td>[Chorus]</td>\n",
       "      <td>Ge-ge-geyeahhh\\nCan I live?\\nCan I live?</td>\n",
       "      <td>Normal</td>\n",
       "      <td>[geyeahhh, live, live]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>JAY-Z</td>\n",
       "      <td>Can I Live</td>\n",
       "      <td>rap</td>\n",
       "      <td>1996</td>\n",
       "      <td>468624</td>\n",
       "      <td>[Verse 2]</td>\n",
       "      <td>My mind is infested with sick thoughts that ci...</td>\n",
       "      <td>Normal</td>\n",
       "      <td>[mind, infested, sick, thought, circle, like, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>JAY-Z</td>\n",
       "      <td>Can I Live</td>\n",
       "      <td>rap</td>\n",
       "      <td>1996</td>\n",
       "      <td>468624</td>\n",
       "      <td>[Chorus]</td>\n",
       "      <td>Can I live?\\nCan I live?\\nCan I live?\\nCan I l...</td>\n",
       "      <td>Normal</td>\n",
       "      <td>[live, live, live, live]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fabolous</td>\n",
       "      <td>Forgive Me Father</td>\n",
       "      <td>rap</td>\n",
       "      <td>2003</td>\n",
       "      <td>4743</td>\n",
       "      <td>[Hook]</td>\n",
       "      <td>Forgive me father for I have sinned\\nBut look ...</td>\n",
       "      <td>Explicit content</td>\n",
       "      <td>[forgive, father, sinned, look, money, spend, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5334073</th>\n",
       "      <td>Alana Springsteen</td>\n",
       "      <td>New Number</td>\n",
       "      <td>country</td>\n",
       "      <td>2022</td>\n",
       "      <td>1</td>\n",
       "      <td>[Chorus]</td>\n",
       "      <td>One that I ain't dial at least a couple thousa...</td>\n",
       "      <td>Normal</td>\n",
       "      <td>[one, dial, least, couple, thousand, time, hea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5334074</th>\n",
       "      <td>Alana Springsteen</td>\n",
       "      <td>New Number</td>\n",
       "      <td>country</td>\n",
       "      <td>2022</td>\n",
       "      <td>1</td>\n",
       "      <td>[Verse 2]</td>\n",
       "      <td>You need a new number and you can't get it fas...</td>\n",
       "      <td>Normal</td>\n",
       "      <td>[need, new, number, fast, enough, cause, wanna...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5334075</th>\n",
       "      <td>Alana Springsteen</td>\n",
       "      <td>New Number</td>\n",
       "      <td>country</td>\n",
       "      <td>2022</td>\n",
       "      <td>1</td>\n",
       "      <td>[Chorus]</td>\n",
       "      <td>One that I ain't dial at least a couple thousa...</td>\n",
       "      <td>Normal</td>\n",
       "      <td>[one, dial, least, couple, thousand, time, hea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5334076</th>\n",
       "      <td>Alana Springsteen</td>\n",
       "      <td>New Number</td>\n",
       "      <td>country</td>\n",
       "      <td>2022</td>\n",
       "      <td>1</td>\n",
       "      <td>[Bridge]</td>\n",
       "      <td>Oh, if you wanna help me out\\nIf you wanna let...</td>\n",
       "      <td>Normal</td>\n",
       "      <td>[wanna, help, wanna, let, easy, care, way, eve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5334077</th>\n",
       "      <td>Alana Springsteen</td>\n",
       "      <td>New Number</td>\n",
       "      <td>country</td>\n",
       "      <td>2022</td>\n",
       "      <td>1</td>\n",
       "      <td>[Verse 3]</td>\n",
       "      <td>You need a new number, one that I don't know\\n...</td>\n",
       "      <td>Normal</td>\n",
       "      <td>[need, new, number, one, know, keep, hanging, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5334078 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    artist              title      tag  year   views  \\\n",
       "0                    JAY-Z         Can I Live      rap  1996  468624   \n",
       "1                    JAY-Z         Can I Live      rap  1996  468624   \n",
       "2                    JAY-Z         Can I Live      rap  1996  468624   \n",
       "3                    JAY-Z         Can I Live      rap  1996  468624   \n",
       "4                 Fabolous  Forgive Me Father      rap  2003    4743   \n",
       "...                    ...                ...      ...   ...     ...   \n",
       "5334073  Alana Springsteen         New Number  country  2022       1   \n",
       "5334074  Alana Springsteen         New Number  country  2022       1   \n",
       "5334075  Alana Springsteen         New Number  country  2022       1   \n",
       "5334076  Alana Springsteen         New Number  country  2022       1   \n",
       "5334077  Alana Springsteen         New Number  country  2022       1   \n",
       "\n",
       "              part                                             lyrics  \\\n",
       "0        [Verse 1]  While I'm watchin' every nigga watchin' me clo...   \n",
       "1         [Chorus]           Ge-ge-geyeahhh\\nCan I live?\\nCan I live?   \n",
       "2        [Verse 2]  My mind is infested with sick thoughts that ci...   \n",
       "3         [Chorus]  Can I live?\\nCan I live?\\nCan I live?\\nCan I l...   \n",
       "4           [Hook]  Forgive me father for I have sinned\\nBut look ...   \n",
       "...            ...                                                ...   \n",
       "5334073   [Chorus]  One that I ain't dial at least a couple thousa...   \n",
       "5334074  [Verse 2]  You need a new number and you can't get it fas...   \n",
       "5334075   [Chorus]  One that I ain't dial at least a couple thousa...   \n",
       "5334076   [Bridge]  Oh, if you wanna help me out\\nIf you wanna let...   \n",
       "5334077  [Verse 3]  You need a new number, one that I don't know\\n...   \n",
       "\n",
       "             explicitness                tokenized, lemmatized, no bad words  \n",
       "0        Explicit content  [watchin, every, watchin, closely, butter, bre...  \n",
       "1                  Normal                             [geyeahhh, live, live]  \n",
       "2                  Normal  [mind, infested, sick, thought, circle, like, ...  \n",
       "3                  Normal                           [live, live, live, live]  \n",
       "4        Explicit content  [forgive, father, sinned, look, money, spend, ...  \n",
       "...                   ...                                                ...  \n",
       "5334073            Normal  [one, dial, least, couple, thousand, time, hea...  \n",
       "5334074            Normal  [need, new, number, fast, enough, cause, wanna...  \n",
       "5334075            Normal  [one, dial, least, couple, thousand, time, hea...  \n",
       "5334076            Normal  [wanna, help, wanna, let, easy, care, way, eve...  \n",
       "5334077            Normal  [need, new, number, one, know, keep, hanging, ...  \n",
       "\n",
       "[5334078 rows x 9 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38421f9a-2565-4907-9574-1447407ad6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics.to_csv('song_lyrics_lematized.csv', index=False) # the stopwords and very frequent and infrequent words are not removed here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60e7277-ab11-47e6-84b5-815dac3c2e48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34bd0ca2-4882-424a-b9ef-7a7e143c0103",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47cb564-9613-4edd-81a0-ad7110a5b59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(lyric_corpus_tokenized[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b7b4849-79db-4f1a-8471-57dd2f6865db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "lyrics = pd.read_csv('song_lyrics_lematized.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f53c63a0-815b-49f4-aab0-e7b8819d19d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "writing to the list: 100%|██████████| 2797631/2797631 [02:04<00:00, 22473.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "['produced', 'irv', 'gotti', 'intro', 'yeah', 'hah', 'yeah', 'roc', 'fella', 'invite', 'you', 'somethin', 'epic', 'you', 'know', 'well', 'hustle', 'out', 'sense', 'hopelessness', 'sort', 'desperation', 'through', 'that', 'desperation', 'become', 'addicted', 'sort', 'like', 'the', 'fiend', 'accustomed', 'servin', 'but', 'feel', 'have', 'nothin', 'lose', 'offer', 'you', 'well', 'offer', 'our', 'life', 'right', 'what', 'you', 'bring', 'the', 'table', 'verse', 'while', 'watchin', 'every', 'nigga', 'watchin', 'closely', 'shit', 'butter', 'for', 'the', 'bread', 'they', 'wanna', 'toast', 'keep', 'head', 'both', 'them', 'where', 'they', 'supposed', 'hoe', 'get', 'you', 'sidetracked', 'then', 'clapped', 'from', 'close', 'foot', 'don', 'sleep', 'tired', 'feel', 'wired', 'like', 'codeine', 'these', 'day', 'brother', 'gotta', 'admire', 'from', 'four', 'fiend', 'away', 'pain', 'wish', 'wa', 'quick', 'see', 'from', 'sellin', 'caine', 'til', 'brain', 'wa', 'fried', 'fricassee', 'can', 'lie', 'the', 'time', 'never', 'bothered', 'the', 'bar', 'gettin', 'thug', 'properly', 'squad', 'and', 'lack', 'respect', 'for', 'authority', 'laughin', 'hard', 'happy', 'escapin', 'poverty', 'however', 'brief', 'know', 'this', 'game', 'got', 'valley', 'and', 'peak', 'expectation', 'for', 'dip', 'for', 'precipitation', 'stack', 'chip', 'hardly', 'the', 'youth', 'used', 'soon', 'see', 'mill', 'more', 'big', 'willie', 'game', 'ha', 'grown', 'prefer', 'you', 'call', 'william', 'illin', 'for', 'revenue', 'rayful', 'edmond', 'like', 'channel', 'news', 'round', 'seven', 'jewel', 'head', 'dead', 'the', 'mic', 'forgettin', 'all', 'ever', 'knew', 'convenient', 'amnesia', 'suggest', 'you', 'call', 'lawyer', 'know', 'the', 'procedure', 'lock', 'body', 'can', 'trap', 'mind', 'easily', 'explain', 'why', 'adapt', 'crime', 'rather', 'die', 'enormous', 'than', 'live', 'dormant', 'that', 'how', 'live', 'the', 'main', 'event', 'bet', 'trip', 'maui', 'presidential', 'suite', 'residential', 'for', 'the', 'weekend', 'confidentially', 'speakin', 'code', 'since', 'sense', 'you', 'peekin', 'the', 'nsx', 'rental', 'don', 'fooled', 'game', 'mental', 'both', 'out', 'town', 'dog', 'what', 'you', 'tryin', 'get', 'into', 'viva', 'la', 'vega', 'see', 'later', 'the', 'crap', 'table', 'meet', 'the', 'one', 'that', 'start', 'this', 'way', 'fraud', 'willies', 'present', 'gamblin', 'they', 'and', 'can', 'have', 'pleasant', 'time', 'sippin', 'margarita', 'chorus', 'geyeahhh', 'can', 'live', 'can', 'live', 'verse', 'mind', 'infested', 'with', 'sick', 'thought', 'that', 'circle', 'like', 'lexus', 'driven', 'wrong', 'sure', 'hurt', 'you', 'dual', 'level', 'like', 'duplex', 'unity', 'crew', 'and', 'commit', 'atrocity', 'like', 'got', 'immunity', 'you', 'guessed', 'manifest', 'tangible', 'good', 'platinum', 'rolex', 'don', 'lease', 'buy', 'the', 'whole', 'car', 'you', 'should', 'confederation', 'dead', 'nation', 'explode', 'detonation', 'overload', 'the', 'mind', 'said', 'patient', 'when', 'boil', 'steam', 'come', 'all', 'fiend', 'gotta', 'even', 'righteous', 'mind', 'through', 'this', 'true', 'this', 'the', 'street', 'school', 'spend', 'our', 'money', 'foolish', 'bond', 'with', 'jeweler', 'and', 'watch', 'for', 'intruder', 'stepped', 'another', 'level', 'meditated', 'like', 'buddhist', 'recruited', 'lieutenant', 'with', 'ludicrous', 'dream', 'gettin', 'cream', 'let', 'this', 'get', 'tedious', 'keep', 'one', 'eye', 'open', 'like', 'cbs', 'you', 'see', 'stressed', 'right', 'chorus', 'can', 'live', 'can', 'live', 'can', 'live', 'can', 'live']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "lyric_corpus_tokenized = list()\n",
    "for song in tqdm(lyrics['lemmatized'], desc=\"writing to the list\"):\n",
    "    lyric_corpus_tokenized.append(song.strip(\"[]\").replace(\"'\", \"\").split(\", \"))\n",
    "print(type(lyric_corpus_tokenized[0]))\n",
    "print(lyric_corpus_tokenized[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6bdfd30c-e134-45cf-9201-e4dc4746ac5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('profanities.txt', 'r') as file:\n",
    "    prof_string = file.read().replace('\\n', '')\n",
    "    profanities = set(prof_string.split(\", \"))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89035211-66a7-4c2c-afa1-0cc437a64830",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "removing profanities: 100%|██████████| 5334078/5334078 [01:03<00:00, 83929.94it/s] \n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def filter_song(song):\n",
    "    return [token for token in song if token not in profanities]\n",
    "\n",
    "lyric_corpus_tokenized = [\n",
    "    filter_song(song) for song in tqdm(lyric_corpus_tokenized, desc=\"removing profanities\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cdc7e64f-797a-4c4d-a5f6-9bded4adab37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196\n",
      "194\n",
      "<class 'set'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "removing stop words: 100%|██████████| 5334078/5334078 [00:56<00:00, 93736.36it/s] \n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "new_stop_words = ['ooh','yeah','hey','whoa','woah', 'ohh', 'was', 'mmm', 'oooh','yah','yeh','mmm', 'hmm','deh','doh','jah','wa']\n",
    "stop_words.extend(new_stop_words)\n",
    "print(len(stop_words))\n",
    "stop_words = set(stop_words)\n",
    "print(len(stop_words))\n",
    "print(type(stop_words))\n",
    "\n",
    "for s,song in enumerate(tqdm(lyric_corpus_tokenized, desc=\"removing stop words\")):\n",
    "    filtered_text = []\n",
    "    for token in song:\n",
    "        if token not in stop_words:\n",
    "            filtered_text.append(token)\n",
    "    lyric_corpus_tokenized[s] = filtered_text\n",
    "\n",
    "# for i in range(10):\n",
    "#     print(lyric_corpus_tokenized[i])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b24258cc-e724-4ca1-b64e-c8f91400e031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lyric_corpus_tokenized = lyrics['no profanities and no stop words'].tolist()\n",
    "lyrics.to_csv('song_lyrics_lematized.csv', index=False) # the stopwords and very frequent and infrequent words are not removed here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fa218a6d-9f93-4cd8-9514-b55c1964b638",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "writing to the list: 100%|██████████| 2797631/2797631 [01:07<00:00, 41462.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "['produced', 'irv', 'gotti', 'intro', 'hah', 'roc', 'fella', 'invite', 'somethin', 'epic', 'know', 'well', 'hustle', 'sense', 'hopelessness', 'sort', 'desperation', 'desperation', 'become', 'addicted', 'sort', 'like', 'fiend', 'accustomed', 'servin', 'feel', 'nothin', 'lose', 'offer', 'well', 'offer', 'life', 'right', 'bring', 'table', 'verse', 'watchin', 'every', 'watchin', 'closely', 'butter', 'bread', 'wanna', 'toast', 'keep', 'head', 'supposed', 'get', 'sidetracked', 'clapped', 'close', 'foot', 'sleep', 'tired', 'feel', 'wired', 'like', 'codeine', 'day', 'brother', 'gotta', 'admire', 'four', 'fiend', 'away', 'pain', 'wish', 'quick', 'see', 'sellin', 'caine', 'til', 'brain', 'fried', 'fricassee', 'lie', 'time', 'never', 'bothered', 'bar', 'gettin', 'properly', 'squad', 'lack', 'respect', 'authority', 'laughin', 'hard', 'happy', 'escapin', 'poverty', 'however', 'brief', 'know', 'game', 'got', 'valley', 'peak', 'expectation', 'dip', 'precipitation', 'stack', 'chip', 'hardly', 'youth', 'used', 'soon', 'see', 'mill', 'big', 'willie', 'game', 'ha', 'grown', 'prefer', 'call', 'william', 'illin', 'revenue', 'rayful', 'edmond', 'like', 'channel', 'news', 'round', 'seven', 'jewel', 'head', 'dead', 'mic', 'forgettin', 'ever', 'knew', 'convenient', 'amnesia', 'suggest', 'call', 'lawyer', 'know', 'procedure', 'lock', 'body', 'trap', 'mind', 'easily', 'explain', 'adapt', 'crime', 'rather', 'die', 'enormous', 'live', 'dormant', 'live', 'main', 'event', 'bet', 'trip', 'maui', 'presidential', 'suite', 'residential', 'weekend', 'confidentially', 'speakin', 'code', 'since', 'sense', 'peekin', 'nsx', 'rental', 'fooled', 'game', 'mental', 'town', 'dog', 'tryin', 'get', 'viva', 'la', 'vega', 'see', 'later', 'table', 'meet', 'one', 'start', 'way', 'fraud', 'present', 'gamblin', 'pleasant', 'time', 'sippin', 'margarita', 'chorus', 'geyeahhh', 'live', 'live', 'verse', 'mind', 'infested', 'sick', 'thought', 'circle', 'like', 'lexus', 'driven', 'wrong', 'sure', 'hurt', 'dual', 'level', 'like', 'duplex', 'unity', 'crew', 'commit', 'atrocity', 'like', 'got', 'immunity', 'guessed', 'manifest', 'tangible', 'good', 'platinum', 'rolex', 'lease', 'buy', 'whole', 'car', 'confederation', 'dead', 'nation', 'explode', 'detonation', 'overload', 'mind', 'said', 'patient', 'boil', 'steam', 'come', 'fiend', 'gotta', 'even', 'righteous', 'mind', 'true', 'street', 'school', 'spend', 'money', 'foolish', 'bond', 'jeweler', 'watch', 'intruder', 'stepped', 'another', 'level', 'meditated', 'like', 'buddhist', 'recruited', 'lieutenant', 'ludicrous', 'dream', 'gettin', 'cream', 'let', 'get', 'tedious', 'keep', 'one', 'eye', 'open', 'like', 'cbs', 'see', 'stressed', 'right', 'chorus', 'live', 'live', 'live', 'live']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# lyrics_corpus_tokenized = list()\n",
    "# for song in tqdm(lyrics['no profanities and no stop words'], desc=\"writing to the list\"):\n",
    "#     lyrics_corpus_tokenized.append(song.strip(\"[]\").replace(\"'\", \"\").split(\", \"))\n",
    "# print(type(lyrics_corpus_tokenized[0]))\n",
    "# print(lyrics_corpus_tokenized[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f36c1a0-ac68-4db9-9f0b-907b3ad08195",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_remove = {'verse', 'chorus', 'hook', 'bridge', 'intro', 'outro', 'pre-chorus', 'pre-hook', 'get', 'got'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cbb6b415-cc65-4223-bccc-c8e29b4d174b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "removing: 100%|██████████| 5334078/5334078 [00:51<00:00, 104157.25it/s]\n"
     ]
    }
   ],
   "source": [
    "lyric_corpus_tokenized = [\n",
    "    [token for token in song if token not in words_to_remove]\n",
    "    for song in tqdm(lyric_corpus_tokenized, desc=\"removing\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "516b50ab-d25b-42e2-8582-c430c046d54a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: gensim in /home/jovyan/.local/lib/python3.8/site-packages (4.3.1)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.8/dist-packages (from gensim) (1.23.1)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.8/dist-packages (from gensim) (1.10.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /home/jovyan/.local/lib/python3.8/site-packages (from gensim) (6.3.0)\n"
     ]
    }
   ],
   "source": [
    "#!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4a28ee86-717b-4022-baea-cd7b7e8aa99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lyrics_corpus_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a3b0f9d0-f953-4228-af20-d63fa6eb9dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating dictionary: 100%|██████████| 5334078/5334078 [03:38<00:00, 24416.18it/s]\n"
     ]
    }
   ],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "# Assuming you have a big list called 'big_list'\n",
    "  # Number of items to choose\n",
    "\n",
    "# Randomly select 20,000 items from the big list\n",
    "# lyric_corpus_tokenized = lyric_corpus_tokenized.tolist()\n",
    "\n",
    "tqdm_corpus = tqdm(lyric_corpus_tokenized, desc=\"Creating dictionary\")\n",
    "dictionary = Dictionary(tqdm_corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "82c95a1d-fee6-469b-a320-162a61c33e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=100, no_above=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0bb441aa-637b-4b28-af57-f655a6404514",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating Gensim Corpus: 100%|██████████| 5334078/5334078 [02:21<00:00, 37624.35it/s]\n"
     ]
    }
   ],
   "source": [
    "from gensim.corpora import MmCorpus\n",
    "gensim_corpus = []\n",
    "with tqdm(total=len(lyric_corpus_tokenized), desc=\"Creating Gensim Corpus\") as pbar:\n",
    "    for song in lyric_corpus_tokenized:\n",
    "        doc = dictionary.doc2bow(song)\n",
    "        gensim_corpus.append(doc)\n",
    "        pbar.update(1)\n",
    "temp = dictionary[0]\n",
    "id2word = dictionary.id2token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "226d9d2e-6672-48ee-a012-e8e0b6245086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accustomed\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a27d1d9d-4570-4382-8859-93746a4d8ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5334078\n"
     ]
    }
   ],
   "source": [
    "print(len(gensim_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a217eeb2-b83e-452e-a15b-b926ce38d8f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 1313.68 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "start_time = time.time()\n",
    "chunksize = 2000\n",
    "passes = 20\n",
    "iterations = 400\n",
    "num_topics = 6\n",
    "sample_size = 2000000\n",
    "\n",
    "\n",
    "gensim2 = random.sample(gensim_corpus, sample_size)\n",
    "\n",
    "lda_model = LdaModel(\n",
    "    corpus=gensim_corpus[],\n",
    "    id2word=id2word,\n",
    "    chunksize=chunksize,\n",
    "    alpha='auto',\n",
    "    eta='auto',\n",
    "    iterations=iterations,\n",
    "    num_topics=num_topics,\n",
    "    passes=passes\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "print(\"Training time: {:.2f} seconds\".format(training_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a8de4d96-c392-42b9-8493-88efa8281482",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting topics:   0%|          | 837/2797631 [00:00<18:53, 2468.12it/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 16583 is out of bounds for axis 1 with size 16583",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tqdm(total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(gensim_corpus), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredicting topics\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m song \u001b[38;5;129;01min\u001b[39;00m gensim_corpus:\n\u001b[0;32m----> 4\u001b[0m         topic_dist \u001b[38;5;241m=\u001b[39m \u001b[43mlda_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_document_topics\u001b[49m\u001b[43m(\u001b[49m\u001b[43msong\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m         predicted_topics\u001b[38;5;241m.\u001b[39mappend(topic_dist)\n\u001b[1;32m      6\u001b[0m         pbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/gensim/models/ldamodel.py:1354\u001b[0m, in \u001b[0;36mLdaModel.get_document_topics\u001b[0;34m(self, bow, minimum_probability, minimum_phi_value, per_word_topics)\u001b[0m\n\u001b[1;32m   1347\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m   1348\u001b[0m         per_word_topics\u001b[38;5;241m=\u001b[39mper_word_topics,\n\u001b[1;32m   1349\u001b[0m         minimum_probability\u001b[38;5;241m=\u001b[39mminimum_probability,\n\u001b[1;32m   1350\u001b[0m         minimum_phi_value\u001b[38;5;241m=\u001b[39mminimum_phi_value\n\u001b[1;32m   1351\u001b[0m     )\n\u001b[1;32m   1352\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply(corpus, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m-> 1354\u001b[0m gamma, phis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbow\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollect_sstats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mper_word_topics\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1355\u001b[0m topic_dist \u001b[38;5;241m=\u001b[39m gamma[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m/\u001b[39m \u001b[38;5;28msum\u001b[39m(gamma[\u001b[38;5;241m0\u001b[39m])  \u001b[38;5;66;03m# normalize distribution\u001b[39;00m\n\u001b[1;32m   1357\u001b[0m document_topics \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   1358\u001b[0m     (topicid, topicvalue) \u001b[38;5;28;01mfor\u001b[39;00m topicid, topicvalue \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(topic_dist)\n\u001b[1;32m   1359\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m topicvalue \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m minimum_probability\n\u001b[1;32m   1360\u001b[0m ]\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/gensim/models/ldamodel.py:706\u001b[0m, in \u001b[0;36mLdaModel.inference\u001b[0;34m(self, chunk, collect_sstats)\u001b[0m\n\u001b[1;32m    704\u001b[0m Elogthetad \u001b[38;5;241m=\u001b[39m Elogtheta[d, :]\n\u001b[1;32m    705\u001b[0m expElogthetad \u001b[38;5;241m=\u001b[39m expElogtheta[d, :]\n\u001b[0;32m--> 706\u001b[0m expElogbetad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpElogbeta\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mids\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    708\u001b[0m \u001b[38;5;66;03m# The optimal phi_{dwk} is proportional to expElogthetad_k * expElogbetad_kw.\u001b[39;00m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;66;03m# phinorm is the normalizer.\u001b[39;00m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;66;03m# TODO treat zeros explicitly, instead of adding epsilon?\u001b[39;00m\n\u001b[1;32m    711\u001b[0m phinorm \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(expElogthetad, expElogbetad) \u001b[38;5;241m+\u001b[39m epsilon\n",
      "\u001b[0;31mIndexError\u001b[0m: index 16583 is out of bounds for axis 1 with size 16583"
     ]
    }
   ],
   "source": [
    "predicted_topics = []\n",
    "with tqdm(total=len(gensim_corpus), desc=\"Predicting topics\") as pbar:\n",
    "    for song in gensim_corpus:\n",
    "        topic_dist = lda_model.get_document_topics(song)\n",
    "        predicted_topics.append(topic_dist)\n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0c089c57-3176-4d2e-8d61-fec55b59572c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model.save(\"lda_model400\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "71a4337f-dae0-435b-900f-c76f187a9958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pyLDAvis in /home/jovyan/.local/lib/python3.8/site-packages (3.4.0)\n",
      "Requirement already satisfied: numpy>=1.22.0 in /usr/local/lib/python3.8/dist-packages (from pyLDAvis) (1.23.1)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from pyLDAvis) (1.10.1)\n",
      "Requirement already satisfied: pandas>=1.3.4 in /usr/local/lib/python3.8/dist-packages (from pyLDAvis) (2.0.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from pyLDAvis) (1.2.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from pyLDAvis) (3.1.2)\n",
      "Requirement already satisfied: numexpr in /usr/local/lib/python3.8/dist-packages (from pyLDAvis) (2.8.4)\n",
      "Requirement already satisfied: funcy in /home/jovyan/.local/lib/python3.8/site-packages (from pyLDAvis) (2.0)\n",
      "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from pyLDAvis) (1.1.3)\n",
      "Requirement already satisfied: gensim in /home/jovyan/.local/lib/python3.8/site-packages (from pyLDAvis) (4.3.1)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from pyLDAvis) (67.7.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.3.4->pyLDAvis) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.3.4->pyLDAvis) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.3.4->pyLDAvis) (2023.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=1.0.0->pyLDAvis) (3.1.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /home/jovyan/.local/lib/python3.8/site-packages (from gensim->pyLDAvis) (6.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/dist-packages (from jinja2->pyLDAvis) (2.1.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/jovyan/.local/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas>=1.3.4->pyLDAvis) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "38834f87-8d74-4478-bdd6-cec360e68f48",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "drop() takes from 1 to 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyLDAvis\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyLDAvis\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgensim_models\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mgensimvis\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m vis_data \u001b[38;5;241m=\u001b[39m \u001b[43mgensimvis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlda_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgensim_corpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdictionary\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m pyLDAvis\u001b[38;5;241m.\u001b[39mdisplay(vis_data)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyLDAvis/gensim_models.py:123\u001b[0m, in \u001b[0;36mprepare\u001b[0;34m(topic_model, corpus, dictionary, doc_topic_dist, **kwargs)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Transforms the Gensim TopicModel and related corpus and dictionary into\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;124;03mthe data structures needed for the visualization.\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;124;03mSee `pyLDAvis.prepare` for **kwargs.\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    122\u001b[0m opts \u001b[38;5;241m=\u001b[39m fp\u001b[38;5;241m.\u001b[39mmerge(_extract_data(topic_model, corpus, dictionary, doc_topic_dist), kwargs)\n\u001b[0;32m--> 123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpyLDAvis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mopts\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyLDAvis/_prepare.py:432\u001b[0m, in \u001b[0;36mprepare\u001b[0;34m(topic_term_dists, doc_topic_dists, doc_lengths, vocab, term_frequency, R, lambda_step, mds, n_jobs, plot_opts, sort_topics, start_index)\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[38;5;66;03m# Quick fix for red bar width bug.  We calculate the\u001b[39;00m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;66;03m# term frequencies internally, using the topic term distributions and the\u001b[39;00m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;66;03m# topic frequencies, rather than using the user-supplied term frequencies.\u001b[39;00m\n\u001b[1;32m    429\u001b[0m \u001b[38;5;66;03m# For a detailed discussion, see: https://github.com/cpsievert/LDAvis/pull/41\u001b[39;00m\n\u001b[1;32m    430\u001b[0m term_frequency \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(term_topic_freq, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 432\u001b[0m topic_info \u001b[38;5;241m=\u001b[39m \u001b[43m_topic_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtopic_term_dists\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtopic_proportion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mterm_frequency\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mterm_topic_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambda_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    435\u001b[0m token_table \u001b[38;5;241m=\u001b[39m _token_table(topic_info, term_topic_freq, vocab, term_frequency, start_index)\n\u001b[1;32m    436\u001b[0m topic_coordinates \u001b[38;5;241m=\u001b[39m _topic_coordinates(mds, topic_term_dists, topic_proportion, start_index)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyLDAvis/_prepare.py:243\u001b[0m, in \u001b[0;36m_topic_info\u001b[0;34m(topic_term_dists, topic_proportion, term_frequency, term_topic_freq, vocab, lambda_step, R, n_jobs, start_index)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;66;03m# Order the terms for the \"default\" view by decreasing saliency:\u001b[39;00m\n\u001b[1;32m    237\u001b[0m default_term_info \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msaliency\u001b[39m\u001b[38;5;124m'\u001b[39m: saliency,\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTerm\u001b[39m\u001b[38;5;124m'\u001b[39m: vocab,\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFreq\u001b[39m\u001b[38;5;124m'\u001b[39m: term_frequency,\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTotal\u001b[39m\u001b[38;5;124m'\u001b[39m: term_frequency,\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCategory\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDefault\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[0;32m--> 243\u001b[0m default_term_info \u001b[38;5;241m=\u001b[39m \u001b[43mdefault_term_info\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msort_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m    \u001b[49m\u001b[43mby\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msaliency\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mascending\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhead\u001b[49m\u001b[43m(\u001b[49m\u001b[43mR\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msaliency\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;66;03m# Rounding Freq and Total to integer values to match LDAvis code:\u001b[39;00m\n\u001b[1;32m    246\u001b[0m default_term_info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFreq\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfloor(default_term_info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFreq\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mTypeError\u001b[0m: drop() takes from 1 to 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "vis_data = gensimvis.prepare(lda_model, gensim_corpus, dictionary)\n",
    "pyLDAvis.display(vis_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "217ad4eb-52a7-4472-814b-708f2a294aac",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vis_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pyLDAvis\u001b[38;5;241m.\u001b[39msave_html(\u001b[43mvis_data\u001b[49m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./Lyrics_LDA_k_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(num_topics) \u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.html\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vis_data' is not defined"
     ]
    }
   ],
   "source": [
    "pyLDAvis.save_html(vis_data, './Lyrics_LDA_k_'+ str(num_topics) +'.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9f9783b6-5e75-4945-8bc5-c493fc71a4a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[31mERROR: Ignored the following versions that require a different python version: 3.4.1 Requires-Python >=3.9\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement pyLDAvis==3.4.1 (from versions: 1.0.0, 1.1.0, 1.2.0, 1.3.0, 1.3.1, 1.3.2, 1.3.3, 1.3.4, 1.3.5, 1.4.0, 1.4.1, 1.5.0, 1.5.1, 2.0.0, 2.1.0, 2.1.1, 2.1.2, 3.0.0, 3.1.0, 3.2.0, 3.2.1, 3.2.2, 3.3.0, 3.3.1, 3.4.0)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for pyLDAvis==3.4.1\u001b[0m\u001b[31m\n",
      "\u001b[0m3.4.0\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas==1.5.3 in /home/jovyan/.local/lib/python3.8/site-packages (1.5.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.8/dist-packages (from pandas==1.5.3) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas==1.5.3) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.20.3 in /usr/local/lib/python3.8/dist-packages (from pandas==1.5.3) (1.23.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/jovyan/.local/lib/python3.8/site-packages (from python-dateutil>=2.8.1->pandas==1.5.3) (1.16.0)\n",
      "2.0.1\n"
     ]
    }
   ],
   "source": [
    "#print(pyLDAvis.__version__)\n",
    "!pip install pyLDAvis==3.4.1\n",
    "print(pyLDAvis.__version__)\n",
    "!pip install pandas==1.5.3\n",
    "import pandas as pd\n",
    "print(pd.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "01d25cf9-2bbf-40f6-9a94-0a6fc1a2f6f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "print(pd.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c9e4814c-38c4-422b-8a2a-f3189a88531c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5c6b4fe4-1e55-4039-aa12-b2f98ceb2000",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "from gensim.corpora import MmCorpus\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "lda_model = LdaModel.load('lda_model400')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9c182836-d41b-4f35-a573-5fc46a0d6385",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyLDAvis\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyLDAvis\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgensim_models\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mgensimvis\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m vis_data \u001b[38;5;241m=\u001b[39m \u001b[43mgensimvis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlda_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgensim_corpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdictionary\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m pyLDAvis\u001b[38;5;241m.\u001b[39mdisplay(vis_data)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#pyLDAvis.save_html(vis_data, './Lyrics_LDA_k_'+ str(num_topics) +'.html')\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyLDAvis/gensim_models.py:122\u001b[0m, in \u001b[0;36mprepare\u001b[0;34m(topic_model, corpus, dictionary, doc_topic_dist, **kwargs)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprepare\u001b[39m(topic_model, corpus, dictionary, doc_topic_dist\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     78\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Transforms the Gensim TopicModel and related corpus and dictionary into\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;124;03m    the data structures needed for the visualization.\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;124;03m    See `pyLDAvis.prepare` for **kwargs.\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     opts \u001b[38;5;241m=\u001b[39m fp\u001b[38;5;241m.\u001b[39mmerge(\u001b[43m_extract_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtopic_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcorpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdictionary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdoc_topic_dist\u001b[49m\u001b[43m)\u001b[49m, kwargs)\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pyLDAvis\u001b[38;5;241m.\u001b[39mprepare(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mopts)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyLDAvis/gensim_models.py:17\u001b[0m, in \u001b[0;36m_extract_data\u001b[0;34m(topic_model, corpus, dictionary, doc_topic_dists)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m gensim\u001b[38;5;241m.\u001b[39mmatutils\u001b[38;5;241m.\u001b[39mismatrix(corpus):\n\u001b[0;32m---> 17\u001b[0m     corpus_csc \u001b[38;5;241m=\u001b[39m \u001b[43mgensim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcorpus2csc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_terms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdictionary\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     19\u001b[0m     corpus_csc \u001b[38;5;241m=\u001b[39m corpus\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/gensim/matutils.py:152\u001b[0m, in \u001b[0;36mcorpus2csc\u001b[0;34m(corpus, num_terms, dtype, num_docs, num_nnz, printprogress)\u001b[0m\n\u001b[1;32m    149\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPROGRESS: at document #\u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, docno)\n\u001b[1;32m    151\u001b[0m \u001b[38;5;66;03m# zip(*doc) transforms doc to (token_indices, token_counts]\u001b[39;00m\n\u001b[0;32m--> 152\u001b[0m doc_indices, doc_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m doc \u001b[38;5;28;01melse\u001b[39;00m ([], [])\n\u001b[1;32m    153\u001b[0m indices\u001b[38;5;241m.\u001b[39mextend(doc_indices)\n\u001b[1;32m    154\u001b[0m data\u001b[38;5;241m.\u001b[39mextend(doc_data)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "vis_data = gensimvis.prepare(lda_model, gensim_corpus, dictionary)\n",
    "pyLDAvis.display(vis_data)\n",
    "#pyLDAvis.save_html(vis_data, './Lyrics_LDA_k_'+ str(num_topics) +'.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "856d62b0-c8be-4fd8-939a-59ba1a5686ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics = pd.read_csv('song_topics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "72c0ff7e-8dc9-456e-abef-a464aab55749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1708442  life and relationships\n",
      "1980937  money and authority\n",
      "1266192  general\n",
      "378507  religion and society\n"
     ]
    }
   ],
   "source": [
    "\n",
    "count = lyrics['topic'].value_counts()['life and relationships']\n",
    "print(count, \" life and relationships\")\n",
    "count = lyrics['topic'].value_counts()['money and authority']\n",
    "print(count, \" money and authority\")\n",
    "count = lyrics['topic'].value_counts()['general']\n",
    "print(count, \" general\")\n",
    "count = lyrics['topic'].value_counts()['religion and society']\n",
    "print(count, \" religion and society\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193d981f-e151-4069-b403-2f2b31a8773e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
